{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Завдання щодо генерації текстів або машинного перекладу (на вибір) на базі рекурентних мереж або трансформерів (на вибір). Вирішіть завдання щодо генерації текстів або машинного перекладу. Особливо вітаються україномовні моделі.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Йди.\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Привіт!\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Біжи!\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Нічого!\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Пожежа!\n",
      "\n",
      "      \n",
      "      \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "with open('ukr.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# розділення на вхідні і цільові тексти\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "for line in lines:\n",
    "    # Розділення рядка на англійський і український текст\n",
    "    parts = line.strip().split('\\t')\n",
    "    if len(parts) >= 2:\n",
    "        input_texts.append(parts[0])  # Вхідні речення англійською\n",
    "        target_texts.append('\\t' + parts[1] + '\\n')  # Цільові речення українською із символами початку і кінця\n",
    "\n",
    "# Створення списків унікальних символів для вхідного і цільового тексту\n",
    "input_characters = sorted(set(''.join(input_texts)))\n",
    "target_characters = sorted(set(''.join(target_texts)))\n",
    "\n",
    "# Визначення розмірності вхідного і цільового словників\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "# Визначення максимальних довжин речень\n",
    "max_encoder_seq_length = max([len(text) for text in input_texts])\n",
    "max_decoder_seq_length = max([len(text) for text in target_texts])\n",
    "\n",
    "# Створення словників символів і їх індексів\n",
    "input_token_index = {char: i for i, char in enumerate(input_characters)}\n",
    "target_token_index = {char: i for i, char in enumerate(target_characters)}\n",
    "reverse_input_char_index = {i: char for char, i in input_token_index.items()}\n",
    "reverse_target_char_index = {i: char for char, i in target_token_index.items()}\n",
    "\n",
    "# Перетворення тексту у вектори one-hot\n",
    "def vectorize_texts(input_texts, target_texts):\n",
    "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "    decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "    decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.0  # Вхідні символи\n",
    "        for t, char in enumerate(target_text):\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.0  # Вхідні символи для декодера\n",
    "            if t > 0:\n",
    "                # Символи для передбачення (зсунутий цільовий текст)\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_texts(input_texts, target_texts)\n",
    "\n",
    "# Архітектура моделі\n",
    "latent_dim = 256  # Розмір прихованого стану\n",
    "\n",
    "# Енкодер: обробка вхідного тексту\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Декодер: генерація тексту\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Повна модель для навчання\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Компіляція моделі\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Навчання моделі\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],  # Вхідні дані\n",
    "    decoder_target_data,  # Цільові дані\n",
    "    batch_size=64,\n",
    "    epochs=30,  # Кількість епох\n",
    "    validation_split=0.2  # Відсоток даних для валідації\n",
    ")\n",
    "\n",
    "# Інференс: окремі моделі для енкодера і декодера\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Декодер для передбачення\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Функція декодування\n",
    "def decode_sequence(input_seq):\n",
    "    # Отримання прихованих станів енкодера\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Початковий символ для декодера\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.0\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        # Передбачення наступного символу\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Декодування символу\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Зупинка, якщо досягнуто кінця речення або максимальної довжини\n",
    "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Оновлення вхідних даних декодера\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(5):  # Перевірка перших 5 речень\n",
    "    input_seq = encoder_input_data[seq_index:seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(f\"Input sentence: {input_texts[seq_index]}\")\n",
    "    print(f\"Decoded sentence: {decoded_sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спершу ми завантажили англо-український датасет із реченнями, розбили його на вхідні (англійські) та цільові (українські) тексти, додавши символи початку та кінця речення для цільових текстів. Ми створили словники унікальних символів для кожної мови, що використовуються для перетворення тексту у вектори one-hot. Вхідні дані були векторизовані у вигляді тривимірних матриць, які мають інформацію про послідовності символів у реченнях. Архітектура моделі включає енкодер на базі LSTM. Декодер, побудований на LSTM, використовує приховані стани енкодера для генерації українського перекладу, символ за символом, через шар із функцією активації softmax. Модель була навчена на 30 епохах із валідаційним розділом 20%, використовуючи функцію втрат categorical_crossentropy і оптимізатор rmsprop. Після навчання ми створили окремі моделі для інференсу: енкодер формує приховані стани для нових речень, а декодер генерує переклад. Наприкінці ми протестували модель на перших 10 реченнях з датасету"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Проведіть експерименти з моделями бібліотеки Hugging Face (раніше - Hugging Face Transformers, https://huggingface.co/) за допомогою (наприклад) Pipeline модуля "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перекладений текст: Великий поет Тарас Шевченко відомий як голос України.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Згенерований текст:\n",
      "Варіант 1: Taras Shevchenko's books: all volumes one through seventy - six ; with great reference to the history of the world, and to a large number of historical documents that could provide insights on history and the world and its impact on human\n",
      "Варіант 2: Taras Shevchenko's books: \n",
      " the'great quest ', book two, the dragon slayer, and his quest : \n",
      " the journey that will end our lives, book three, the greatest book of all time, books five,\n",
      "Варіант 3: Taras Shevchenko's books: \n",
      " the adventures of the klansmen \n",
      " two black - jacketed bandits \n",
      " the golden bough episode 1 \n",
      " the secrets to ancient secrets \n",
      " the great seal \n",
      " the iron and gold collection \n",
      " and the\n",
      "Варіант 4: Taras Shevchenko's books: \n",
      " the world is ruled by a monster : a world that destroys everything except for its own kind of evil. \n",
      " and for this, the writer must choose which character to be. for it has already\n",
      "Варіант 5: Taras Shevchenko's books: \n",
      " - the tale of two sisters in the mountains : \n",
      " - the tale of a woman with a strange voice. \n",
      " - the story of a dog ; \n",
      " - the story of a mountain and an\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Завантаження моделі перекладу (англійська -> українська)\n",
    "translation_pipeline = pipeline(\"translation_en_to_uk\", model=\"Helsinki-NLP/opus-mt-en-uk\")\n",
    "text_to_translate = \"The great poet Taras Shevchenko is known as the voice of Ukraine.\"\n",
    "translated_text = translation_pipeline(text_to_translate)\n",
    "print(\"Перекладений текст:\", translated_text[0]['translation_text'])\n",
    "\n",
    "# Завантаження моделі для генерації тексту\n",
    "generator = pipeline('text-generation', model='openai-gpt') \n",
    "\n",
    "# Генерація тексту\n",
    "prompt = \"Taras Shevchenko's books:\"\n",
    "generated_texts = generator(prompt, max_length=50, num_return_sequences=5)\n",
    "\n",
    "# Виведення результатів\n",
    "print(\"\\nЗгенерований текст:\")\n",
    "for i, output in enumerate(generated_texts, 1):\n",
    "    print(f\"Варіант {i}: {output['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перша задача полягала у перекладі тексту з англійської на українську за допомогою моделі Helsinki-NLP/opus-mt-en-uk. Результат був точним і відповідав очікуванням: фраза \"The great poet Taras Shevchenko is known as the voice of Ukraine\" була коректно перекладена як \"Великий поет Тарас Шевченко відомий як голос України\". \n",
    "\n",
    "Друга задача стосувалася генерації тексту англійською мовою за допомогою моделі openai-gpt. На жаль, результати генерації виявилися менш задовільними: модель створювала текст, який не мав логічного зв’язку з заданим контекстом про творчість Тараса Шевченка. Наприклад, у згенерованих фрагментах з’являлися вигадані назви книг (\"dragon slayer,\" \"tale of two sisters\"), які не мають жодного стосунку до українського поета. Це вказує на обмеженість цієї моделі у розумінні культурно-специфічного контексту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Завдання щодо генерації або стилізації зображень (на вибір)\n",
    "Вирішіть завдання перенесення стилю або генерації зображень (архітектура за вашим вибором: GAN/DCGAN/VAE/Diffusion). Датасети: можна брати CIFAR-100, Fashion MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Налаштування пристрою: вибір між GPU та CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Розмір партії для навчання (batch size)\n",
    "bs = 100\n",
    "\n",
    "# Завантаження та підготовка даних MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Нормалізація даних у діапазоні [-1, 1], щоб полегшити навчання мережі\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# Перетворення зображень у вектори довжиною 28*28 (плоске зображення)\n",
    "x_train = torch.tensor(x_train.reshape(-1, 28 * 28))\n",
    "x_test = torch.tensor(x_test.reshape(-1, 28 * 28))\n",
    "\n",
    "# Створення DataLoader для тренувальних та тестових даних\n",
    "train_loader = torch.utils.data.DataLoader(x_train, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(x_test, batch_size=bs, shuffle=False)\n",
    "\n",
    "# Опис архітектури генератора\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, g_input_dim, g_output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        # Шари мережі для генератора\n",
    "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features * 2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features * 2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Нелінійність Leaky ReLU для кожного шару\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        # Використовуємо Tanh для нормалізації вихідних значень в діапазоні [-1, 1]\n",
    "        return torch.tanh(self.fc4(x))\n",
    "\n",
    "# Опис архітектури дискримінатора\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Шари мережі для дискримінатора\n",
    "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features // 2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features // 2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Нелінійність Leaky ReLU і використання Dropout для зменшення перенавчання\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        # Використовуємо сигмоїду для видачі ймовірностей\n",
    "        return torch.sigmoid(self.fc4(x))\n",
    "\n",
    "# Ініціалізація генератора і дискримінатора\n",
    "z_dim = 100  # Розмір латентного простору (вхідний шум)\n",
    "mnist_dim = 28 * 28  # Розмір зображення 28x28 (плоске)\n",
    "\n",
    "# Моделі генератора та дискримінатора\n",
    "G = Generator(g_input_dim=z_dim, g_output_dim=mnist_dim).to(device)\n",
    "D = Discriminator(mnist_dim).to(device)\n",
    "\n",
    "# Визначення функції втрат (BCELoss) і оптимізаторів (Adam)\n",
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Функція тренування дискримінатора\n",
    "def D_train(x):\n",
    "    D.zero_grad()\n",
    "    # Навчання на реальних даних\n",
    "    x_real, y_real = x.view(-1, mnist_dim).to(device), torch.ones(bs, 1).to(device)\n",
    "    D_output = D(x_real)\n",
    "    D_real_loss = criterion(D_output, y_real)\n",
    "\n",
    "    # Навчання на фейкових даних\n",
    "    z = torch.randn(bs, z_dim).to(device)\n",
    "    x_fake, y_fake = G(z), torch.zeros(bs, 1).to(device)\n",
    "    D_output = D(x_fake)\n",
    "    D_fake_loss = criterion(D_output, y_fake)\n",
    "\n",
    "    # Обчислення загальних втрат та оновлення ваг\n",
    "    D_loss = D_real_loss + D_fake_loss\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "    return D_loss.item()\n",
    "\n",
    "# Функція тренування генератора\n",
    "def G_train():\n",
    "    G.zero_grad()\n",
    "    z = torch.randn(bs, z_dim).to(device)\n",
    "    y = torch.ones(bs, 1).to(device)  # Генератор хоче, щоб дискримінатор думав, що це справжні зображення\n",
    "    G_output = G(z)\n",
    "    D_output = D(G_output)\n",
    "    G_loss = criterion(D_output, y)\n",
    "\n",
    "    # Оновлення ваг генератора\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "    return G_loss.item()\n",
    "\n",
    "n_epoch = 100 \n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    D_losses, G_losses = [], []\n",
    "    for batch_idx, x in enumerate(train_loader):\n",
    "        D_losses.append(D_train(x))  # Тренування дискримінатора\n",
    "        G_losses.append(G_train())  # Тренування генератора\n",
    "\n",
    "# Генерація фінального результату після тренування\n",
    "with torch.no_grad():\n",
    "    final_z = torch.randn(bs, z_dim).to(device)  \n",
    "    generated = G(final_z)  # Генерація зображень\n",
    "    save_image(generated.view(bs, 1, 28, 28), 'final_result.png') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат було збережно у final_result.png ![Result](lb4/final_result.png)\n",
    "\n",
    "Мережа складається з двох частин: генератора, який генерує нові зображення, та дискримінатора, який оцінює, чи є зображення реальними чи фейковими. Генератор намагається створити зображення, що нагадують цифри з MNIST, в той час як дискримінатор намагається визначити, чи є ці зображення реальними чи фейковими.\n",
    "\n",
    "Результат є досить хорошим, оскільки згенеровані зображення схожі на цифри з набору MNIST. Хоча є деякі незначні дефекти та шуми, загальна якість зображень демонструє, що генеративна модель здатна добре імітувати цифри, яких вона не бачила під час навчання."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
